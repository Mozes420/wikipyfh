{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Dump Parser: wdp.py\n",
    "\n",
    "For the kWiki project, the already existing \"wiki dump parser\" was adapted: https://github.com/Grasia/wiki-scripts/tree/master/wiki_dump_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages used:\n",
    "  \n",
    "**xml.parsers.expat:** https://docs.python.org/3/library/pyexpat.html  \n",
    "**xyx** https://docs.python.org/3/library/sys.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "  wiki_dump_parser.py\n",
    "  Script to convert a xml mediawiki history dump to a csv file with readable useful data\n",
    "for pandas processing.\n",
    "  Copyright 2017-2019 Abel 'Akronix' Serrano Juste <akronix5@gmail.com>\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "   adapted for use in the \"kWiki\" project by 'Media Group4 / BDS 19, Sem5'\n",
    "   Nicolas Koch, Moritz Schl√∂gel, Moritz Wieser, Nina Pasku\n",
    "\"\"\"\n",
    "\n",
    "import xml.parsers.expat\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyrsistent import l\n",
    "\n",
    "__version__ = '2.0.2'\n",
    "\n",
    "Debug = False\n",
    "\n",
    "csv_separator = \"#\"\n",
    "\n",
    "def xml_to_csv(filename):\n",
    "\n",
    "  ### BEGIN xmt_to_csv var declarations ###\n",
    "  # Shared variables for parser subfunctions:\n",
    "  ## output_csv, _current_tag, _parent\n",
    "  ## page_id,page_title,revision_id,timestamp,contributor_id,contributor_name,bytes_var\n",
    "\n",
    "  output_csv = None\n",
    "  _parent = None\n",
    "  _current_tag = ''\n",
    "  page_id = page_title = revision_id = timestamp = comment = contributor_id = contributor_name = bytes_var = revtext = ''\n",
    "  # bds: added comment and revtext\n",
    "\n",
    "  def start_tag(tag, attrs):\n",
    "    nonlocal output_csv, _current_tag, _parent\n",
    "    nonlocal bytes_var\n",
    "\n",
    "    _current_tag = tag\n",
    "\n",
    "    if tag == 'text':\n",
    "      if 'bytes' in attrs:\n",
    "        bytes_var = attrs['bytes']\n",
    "      else: # There's a 'deleted' flag or no info about bytes of the edition\n",
    "        bytes_var = '1'\n",
    "    elif tag == 'page' or tag == 'revision' or tag == 'contributor':\n",
    "      _parent = tag\n",
    "\n",
    "    if tag == 'upload':\n",
    "      print(\"!! Warning: '<upload>' element not being handled\", file=sys.stderr)\n",
    "\n",
    "  def data_handler(data):\n",
    "    nonlocal output_csv, _current_tag, _parent\n",
    "    nonlocal page_id,page_title,revision_id,timestamp,comment,contributor_id,contributor_name,bytes_var,revtext\n",
    "    toreplace = ['\\n', '|']\n",
    "    pattern = '[' +  ''.join(toreplace) +  ']'\n",
    "  # bds: added comment and revtext and added regex-filter to handle in-text-characters\n",
    "\n",
    "    if _current_tag == '': # Don't process blank \"orphan\" data between tags!!\n",
    "      return\n",
    "\n",
    "    if _parent:\n",
    "      if _parent == 'page':\n",
    "        if _current_tag == 'title':\n",
    "          page_title = '|' + re.sub(pattern, '', data) + '|'\n",
    "        elif _current_tag == 'id':\n",
    "          page_id = data\n",
    "          if Debug:\n",
    "            print(\"Parsing page \" + page_id )\n",
    "      elif _parent == 'revision':\n",
    "        if _current_tag == 'id':\n",
    "          revision_id = data\n",
    "        elif _current_tag == 'timestamp':\n",
    "          timestamp = data\n",
    "        elif _current_tag == 'comment':\n",
    "          comment = '|' + re.sub(pattern, '', data) + '|'\n",
    "        elif _current_tag == 'text':\n",
    "          revtext = '|' + re.sub(pattern, '', data) + '|'\n",
    "      elif _parent == 'contributor':\n",
    "        if _current_tag == 'id':\n",
    "          contributor_id = data\n",
    "        elif _current_tag == 'username':\n",
    "          contributor_name = '|' + re.sub(pattern, '', data) + '|'\n",
    "        elif _current_tag == 'ip':\n",
    "          contributor_id = '|' + re.sub(pattern, '', data) + '|'\n",
    "        \n",
    "\n",
    "  def end_tag(tag):\n",
    "    nonlocal output_csv, _current_tag, _parent\n",
    "    nonlocal page_id,page_title,revision_id,timestamp,comment,contributor_id,contributor_name,bytes_var,revtext\n",
    "\n",
    "\n",
    "    def has_empty_field(l):\n",
    "      field_empty = False;\n",
    "      i = 0\n",
    "      while (not field_empty and i<len(l)):\n",
    "        field_empty = (l[i] == '');\n",
    "        i = i + 1\n",
    "      return field_empty\n",
    "\n",
    "\n",
    "    # uploading one level of parent if any of these tags close\n",
    "    if tag == 'page':\n",
    "      _parent = None\n",
    "    elif tag == 'revision':\n",
    "      _parent = 'page'\n",
    "    elif tag == 'contributor':\n",
    "      _parent = 'revision'\n",
    "\n",
    "    # print revision to revision output csv\n",
    "    if tag == 'revision':\n",
    "\n",
    "      revision_row = [page_id, page_title, revision_id, timestamp, comment, contributor_id, contributor_name, bytes_var, revtext]\n",
    "      rev_row = [comment, contributor_name, revtext]\n",
    "\n",
    "      # Do not print (skip) revisions that has any of the fields not available\n",
    "      if not has_empty_field(revision_row):\n",
    "        output_csv.write(csv_separator.join(revision_row) + '\\n')\n",
    "      elif has_empty_field(rev_row):\n",
    "        output_csv.write(csv_separator.join(revision_row) + '\\n')\n",
    "      else:\n",
    "        print(\"The following line has incomplete info and therefore it's been removed from the dataset:\")\n",
    "        print(revision_row)\n",
    "\n",
    "      # Debug lines to standard output\n",
    "      if Debug:\n",
    "        print(csv_separator.join(revision_row))\n",
    "\n",
    "      # Clearing data that has to be recalculated for every row:\n",
    "      revision_id = timestamp = comment = contributor_id = contributor_name = bytes_var = revtext = ''\n",
    "\n",
    "    _current_tag = '' # Very important!!! Otherwise blank \"orphan\" data between tags remain in _current_tag and trigger data_handler!! >:(\n",
    "\n",
    "\n",
    "  ### BEGIN xml_to_csv body ###\n",
    "\n",
    "  # Initializing xml parser\n",
    "  parser = xml.parsers.expat.ParserCreate()\n",
    "  input_file = open(filename, 'rb')\n",
    "\n",
    "  parser.StartElementHandler = start_tag\n",
    "  parser.EndElementHandler = end_tag\n",
    "  parser.CharacterDataHandler = data_handler\n",
    "  parser.buffer_text = True\n",
    "  parser.buffer_size = 1024\n",
    "\n",
    "  # writing header for output csv file\n",
    "  output_csv = open(filename[0:-3]+\"csv\",'w', newline = '\\n', encoding='utf8')\n",
    "  output_csv.write(csv_separator.join([\"page_id\",\"page_title\",\"revision_id\",\"timestamp\",\"comment\",\"contributor_id\",\"contributor_name\",\"bytes\",\"revtext\"]))\n",
    "  output_csv.write(\"\\n\")\n",
    "\n",
    "  # Parsing xml and writting proccesed data to output csv\n",
    "  print(\"Processing...\")\n",
    "  parser.ParseFile(input_file)\n",
    "  print(\"Done processing\")\n",
    "\n",
    "  input_file.close()\n",
    "  output_csv.close()\n",
    "\n",
    "  return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  if(len(sys.argv)) >= 2:\n",
    "    print ('Dump files to process: {}'.format(sys.argv[1:]))\n",
    "    for xmlfile in sys.argv[1:]:\n",
    "      print(\"Starting to parse file \" + xmlfile)\n",
    "      if xml_to_csv(xmlfile):\n",
    "        print(\"Data dump {} parsed succesfully\".format(xmlfile))\n",
    "  else:\n",
    "    print(\"Error: Invalid number of arguments. Please specify one or more .xml file to parse\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reading in with the parser, the files are required in decompressed XML format.   \n",
    "Decompression is possible using various free tools, such as 7Zip.   \n",
    "For illustration purposes and to avoid long processing times, only the decompressed file `enwiki-p1p857.xml` is used in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge in adapting the parser was that it was not created for processing text content. It was therefore necessary to find a way to bypass (replace) special characters in such a way that the content meaning of the revision texts did not suffer.   \n",
    "We therefore decided on the approach of using the hashtag as the CSV separator and replacing page breaks as well as tubes within the given quotation marks, which in this case were also tubes, with a space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the parser, small changes also had to be made:    \n",
    "For example, the CSV could not be read correctly if the parameter `engine=python` was not passed.   \n",
    "Furthermore, due to the addition of the comments and the revision texts, the parameter `on_bad_lines=warn` had to be used instead of `=error` to avoid that lines without content in these columns would be omitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finished code to retrieve the parser then read: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wdp as parser\n",
    "parser.xml_to_csv('enwiki-20220201-pages-meta-history1.xml-p1p857.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the into a CSV file converted XML file is read in with the specified parameters and can now be further worked on as Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>comment</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>contributor_name</th>\n",
       "      <th>bytes</th>\n",
       "      <th>revtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>233192</td>\n",
       "      <td>2001-01-21 02:12:21</td>\n",
       "      <td>*</td>\n",
       "      <td>99</td>\n",
       "      <td>RoseParks</td>\n",
       "      <td>124</td>\n",
       "      <td>This subject covers* AssistiveTechnology* Acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>862220</td>\n",
       "      <td>2002-02-25 15:43:11</td>\n",
       "      <td>Automated conversion</td>\n",
       "      <td>1226483</td>\n",
       "      <td>Conversion script</td>\n",
       "      <td>35</td>\n",
       "      <td>#REDIRECT [[Accessible Computing]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>15898945</td>\n",
       "      <td>2003-04-25 22:18:38</td>\n",
       "      <td>Fixing redirect</td>\n",
       "      <td>7543</td>\n",
       "      <td>Ams80</td>\n",
       "      <td>34</td>\n",
       "      <td>#REDIRECT [[Accessible_computing]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>56681914</td>\n",
       "      <td>2006-06-03 16:55:41</td>\n",
       "      <td>fix double redirect</td>\n",
       "      <td>516514</td>\n",
       "      <td>Nzd</td>\n",
       "      <td>36</td>\n",
       "      <td>#REDIRECT [[Computer accessibility]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>AccessibleComputing</td>\n",
       "      <td>74466685</td>\n",
       "      <td>2006-09-08 04:16:04</td>\n",
       "      <td>cat rd</td>\n",
       "      <td>750223</td>\n",
       "      <td>Rory096</td>\n",
       "      <td>57</td>\n",
       "      <td>#REDIRECT [[Computer accessibility]] {{R from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id           page_title  revision_id           timestamp  \\\n",
       "0       10  AccessibleComputing       233192 2001-01-21 02:12:21   \n",
       "1       10  AccessibleComputing       862220 2002-02-25 15:43:11   \n",
       "2       10  AccessibleComputing     15898945 2003-04-25 22:18:38   \n",
       "3       10  AccessibleComputing     56681914 2006-06-03 16:55:41   \n",
       "4       10  AccessibleComputing     74466685 2006-09-08 04:16:04   \n",
       "\n",
       "                comment contributor_id   contributor_name  bytes  \\\n",
       "0                     *             99          RoseParks    124   \n",
       "1  Automated conversion        1226483  Conversion script     35   \n",
       "2       Fixing redirect           7543              Ams80     34   \n",
       "3   fix double redirect         516514                Nzd     36   \n",
       "4                cat rd         750223            Rory096     57   \n",
       "\n",
       "                                             revtext  \n",
       "0  This subject covers* AssistiveTechnology* Acce...  \n",
       "1                 #REDIRECT [[Accessible Computing]]  \n",
       "2                 #REDIRECT [[Accessible_computing]]  \n",
       "3               #REDIRECT [[Computer accessibility]]  \n",
       "4  #REDIRECT [[Computer accessibility]] {{R from ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('enwiki-p1p857.csv', quotechar='|', sep = '#', engine = 'python', on_bad_lines='warn')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'],format='%Y-%m-%dT%H:%M:%SZ')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
